{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for logistic matrix factorization \n",
    "Paper https://www.zora.uzh.ch/id/eprint/150303/1/two-class-cf.pdf\n",
    "\n",
    "Converted code from theano: https://github.com/uzh/tccf/blob/master/softmax_mf_theano.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable as V\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Class for Logistic Model \n",
    "class LogisticMF(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_ccs, num_items, num_factors, alpha):\n",
    "        super(LogisticMF, self).__init__()\n",
    "        \n",
    "        ### Embeddings for the ccs and items\n",
    "        self.ccs = nn.Embedding(num_ccs, num_factors)\n",
    "        self.item = nn.Embedding(num_items, num_factors)\n",
    "        ### bias for the ccs and items \n",
    "        self.ccs_bias = nn.Embedding(num_ccs, 1)\n",
    "        self.item_bias = nn.Embedding(num_items, 1)      \n",
    "        \n",
    "        ### initialization of the weights\n",
    "        self.ccs.weight.data.uniform_(-.01, .01)\n",
    "        self.item.weight.data.uniform_(-.01, .01)\n",
    "        self.ccs_bias.weight.data.uniform_(-.01, .01)\n",
    "        self.item_bias.weight.data.uniform_(-.01, .01)\n",
    "        \n",
    "        ### regularization factor\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, pairs, target = None, loss_func = None):\n",
    "        ### pairs is an Nx2 tensor that contains ccs, item pairs\n",
    "        codes, features = pairs[:,0], pairs[:,1]\n",
    "        c, it = self.ccs(codes), self.item(features)\n",
    "        estimated_matrix = (c*it).sum(1)\n",
    "        estimated_matrix_b = estimated_matrix + self.ccs_bias(codes).squeeze() + self.item_bias(features).squeeze()\n",
    "        if target == None:\n",
    "            return estimated_matrix_b\n",
    "        else:\n",
    "            loss = loss_func(estimated_matrix_b, target)\n",
    "            l2_reg = self.alpha / 2 * (torch.norm(model.ccs.weight, p = 2) + torch.norm(model.item.weight, p = 2))\n",
    "            loss += l2_reg\n",
    "            return loss\n",
    "\n",
    "### softmax matrix factorization\n",
    "class S_MF(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_ccs, num_items, num_factors, alpha):\n",
    "        super(S_MF, self).__init__()\n",
    "\n",
    "        ### Embeddings for the ccs and items\n",
    "        self.ccs = nn.Embedding(num_ccs, num_factors)\n",
    "        self.item_pos = nn.Embedding(num_items, num_factors)\n",
    "        self.item_neg = nn.Embedding(num_items, num_factors)\n",
    "\n",
    "        ### bias for the ccs and items \n",
    "        self.ccs_bias = nn.Embedding(num_ccs, 1)\n",
    "        self.item_bias_pos = nn.Embedding(num_items, 1) \n",
    "        self.item_bias_neg = nn.Embedding(num_items, 1)      \n",
    "\n",
    "        ### initialization of the weights\n",
    "        self.ccs.weight.data.uniform_(-.01, .01)\n",
    "        self.item_pos.weight.data.uniform_(-.01, .01)\n",
    "        self.item_neg.weight.data.uniform_(-.01, .01)\n",
    "        self.ccs_bias.weight.data.uniform_(-.01, .01)\n",
    "        self.item_bias_pos.weight.data.uniform_(-.01, .01)\n",
    "        self.item_bias_neg.weight.data.uniform_(-.01, .01)\n",
    "        \n",
    "        ### regularization factor\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, pairs, target = None):\n",
    "        ### pairs is an Nx2 tensor that contains ccs, item pairs\n",
    "        codes, features = pairs[:,0], pairs[:,1]\n",
    "        c, it_pos, it_neg = self.ccs(codes), self.item_pos(features), self.item_neg(features)\n",
    "        matrix_pos = (c*it_pos).sum(1)\n",
    "        matrix_neg = (c*it_neg).sum(1)\n",
    "        matrix_pos_b = matrix_pos + self.ccs_bias(codes).squeeze() + self.item_bias_pos(features).squeeze()\n",
    "        matrix_neg_b = matrix_neg + self.ccs_bias(codes).squeeze() + self.item_bias_neg(features).squeeze()\n",
    "        if target == None:\n",
    "            return matrix_pos_b, matrix_neg_b    \n",
    "        else:\n",
    "            delta = nn.Threshold(0.5,0)\n",
    "            pos = delta(target)\n",
    "            neg = delta(-target)\n",
    "            loss = (-pos * matrix_pos_b - neg * matrix_neg_b + torch.log(1 + torch.exp(matrix_pos_b) + torch.exp(matrix_neg_b))).sum()\n",
    "            l2 = self.alpha / 3 * (torch.norm(model.ccs.weight, p = 2) + torch.norm(model.item_pos.weight, p = 2) + torch.norm(model.item_neg.weight, p = 2))\n",
    "            loss = loss + l2 \n",
    "            return loss\n",
    "\n",
    "### loss functions for logistic matrix factorization\n",
    "def loss_tc_mf1(output, target):\n",
    "    delta = nn.Threshold(0.5, 0)\n",
    "    pos = delta(target)\n",
    "    loss = (-pos * output + torch.log(1 + torch.exp(output))).sum()\n",
    "    return loss\n",
    "\n",
    "def loss_tc_mf(output, target):\n",
    "    delta = nn.Threshold(0.5, 0)\n",
    "    pos = delta(target)\n",
    "    neg = delta(-target)\n",
    "    indicator = pos + neg\n",
    "    loss = (-pos * output + indicator * torch.log(1 + torch.exp(output))).sum()\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 10] loss: 0.9430566326350345\n",
      "[2, 10] loss: 0.7025096127553297\n",
      "[3, 10] loss: 0.45789282619953153\n",
      "[4, 10] loss: 0.34755476713180544\n",
      "[5, 10] loss: 0.2955535993100057\n",
      "[6, 10] loss: 0.2511948686093092\n",
      "[7, 10] loss: 0.22838901025243102\n",
      "[8, 10] loss: 0.19627347010391533\n",
      "[9, 10] loss: 0.17505496962112374\n",
      "[10, 10] loss: 0.15911585781723261\n",
      "[11, 10] loss: 0.1386752274062019\n",
      "[12, 10] loss: 0.12007428063661792\n",
      "[13, 10] loss: 0.10150305233546533\n",
      "[14, 10] loss: 0.08129603624693119\n",
      "[15, 10] loss: 0.0654752429574728\n",
      "[16, 10] loss: 0.05254608520772308\n",
      "[17, 10] loss: 0.04468314724508673\n",
      "[18, 10] loss: 0.03632223354652524\n",
      "[19, 10] loss: 0.031832817499525844\n",
      "[20, 10] loss: 0.027891686710063368\n",
      "[21, 10] loss: 0.02492203686852008\n",
      "[22, 10] loss: 0.022577126056421547\n",
      "[23, 10] loss: 0.021087916928809135\n",
      "[24, 10] loss: 0.019579565222375094\n",
      "[25, 10] loss: 0.01847477356204763\n",
      "[26, 10] loss: 0.017607847461476922\n",
      "[27, 10] loss: 0.016597243305295706\n",
      "[28, 10] loss: 0.0158624772913754\n",
      "[29, 10] loss: 0.015246757760178298\n",
      "[30, 10] loss: 0.014610065566375851\n",
      "[31, 10] loss: 0.014025461801793426\n",
      "[32, 10] loss: 0.01358405309729278\n",
      "[33, 10] loss: 0.013090320164337754\n",
      "[34, 10] loss: 0.012679986108560115\n",
      "[35, 10] loss: 0.012372768949717283\n",
      "[36, 10] loss: 0.012029653880745173\n",
      "[37, 10] loss: 0.011637072917073965\n",
      "[38, 10] loss: 0.011343359248712659\n",
      "[39, 10] loss: 0.01104333730181679\n",
      "[40, 10] loss: 0.010867399000562727\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "### dump example to run the above models\n",
    "n_epoch = 40\n",
    "lr = 0.1\n",
    "alpha = 0.0005 \n",
    "training_loss = []\n",
    "\n",
    "### should be 'smf' or 'logistic_mf'\n",
    "used_model = 'logistic_mf'\n",
    "### loss_tc_mf1 or loss_tc_mf. Used only if 'logstic_mf' model will be used. \n",
    "loss_func = loss_tc_mf\n",
    "if loss_func not in [loss_tc_mf, loss_tc_mf1]:\n",
    "    raise Exception(\"loss_func variable got {}, but it accepts only loss_tc_mf or loss_tc_mf1\".format(loss_func))\n",
    "\n",
    "\n",
    "# create model and optimizer\n",
    "num_ccs = 4\n",
    "num_items = 5\n",
    "num_factors = 3\n",
    "\n",
    "\n",
    "x = torch.tensor([[1, 0, -1, 0, 1], [1, 1, 0, 1, 1], [-1, -1, 0, 0 ,0], [1,1,-1,-1,-1]])\n",
    "df = pd.DataFrame(data=x.numpy())\n",
    "x_train = df.stack().reset_index().rename(columns={'level_0':'ccs_idx','level_1':'item_idx', 0:'rating'}).to_numpy()\n",
    "\n",
    "if used_model == 'smf':\n",
    "    model = S_MF(num_ccs, num_items, num_factors, alpha).cuda()\n",
    "elif used_model == 'logistic_mf':\n",
    "    model = LogisticMF(num_ccs, num_items, num_factors, alpha).cuda()\n",
    "else:\n",
    "    raise Exception(\"used_model variable got {}, but it accepts only smf or logistic_mf\".format(used_model))\n",
    "    \n",
    "### weight decay is not used because the l2 regularization was used in the loss functions \n",
    "opt = optim.SGD(model.parameters(), lr, momentum=0.9)\n",
    "\n",
    "train_loader = DataLoader(x_train, batch_size=2, shuffle=True)\n",
    "for epoch in range(n_epoch):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        # get the inputs\n",
    "        inputs = batch.long().cuda()\n",
    "        true_val = V(batch[:, 2].float()).cuda()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        if used_model == 'logistic_mf':\n",
    "            loss = model.forward(inputs, true_val, loss_func)\n",
    "        else: \n",
    "            loss = model.forward(inputs, true_val)\n",
    "    \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % (len(train_loader)) == (len(train_loader) - 1):    # print every 2000 mini-batches\n",
    "            training_loss.append((running_loss/len(train_loader)))\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss/len(train_loader)}\")\n",
    "            #epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  0, -1,  0,  1],\n",
      "        [ 1,  1,  0,  1,  1],\n",
      "        [-1, -1,  0,  0,  0],\n",
      "        [ 1,  1, -1, -1, -1]])\n",
      "tensor([[-1.3516, -1.8634, -0.6829,  1.5519,  2.4966],\n",
      "        [-0.8808, -1.1887, -0.4041,  0.9894,  1.5676],\n",
      "        [-1.5965, -2.1444, -0.7183,  1.7818,  2.8163],\n",
      "        [ 3.2821,  4.4213,  1.4949, -3.6769, -5.8208]], device='cuda:0',\n",
      "       grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "###check the estimated matrix comparing to the target one\n",
    "print(x)\n",
    "if used_model == 'smf':\n",
    "    print(torch.matmul(model.ccs.weight, torch.transpose(model.item_pos.weight + model.item_neg.weight, 0, 1)))\n",
    "elif used_model == 'logistic_mf':\n",
    "    print(torch.matmul(model.ccs.weight, torch.transpose(model.item.weight, 0, 1)))\n",
    "else:\n",
    "    raise Exception(\"used_model variable got {}, but it accepts only smf or logistic_mf\".format(used_model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
